# Wikipedia Clickstream Analysis with PySpark

## ğŸ“š Project Overview

This project analyzes a sample of the English-language **Wikipedia Clickstream dataset from January 2018** using **PySpark**. The dataset tracks how users arrive at specific Wikipedia articles â€” whether via search engines, internal links, or other mechanisms.

One example from the dataset:  
> Approximately **37,000 users** accessed the page for **London** from an **external search engine**, while about **4,600** arrived by clicking a link from the article titled **"United Kingdom"**.

The project demonstrates how large-scale web interaction data can be explored efficiently using distributed data processing tools.

[Click](/Program%20and%20File/notebook.ipynb) to check out the results on a Junyper file.

![asset](https://static-assets.codecademy.com/Courses/big-data-pyspark/sql-project-sankey.svg)

---

## ğŸ›  Tools & Technologies

- Apache Spark (PySpark)
- Python 3.x
- Jupyter Notebook or Databricks (optional)
- Dataset: [Wikipedia Clickstream January 2018 (English)](https://dumps.wikimedia.org/other/clickstream/2018-01/)

---

## ğŸ“ Dataset Description

Each row in the dataset represents aggregated click counts from one Wikipedia article (the "referrer") to another (the "resource").

Columns:
- `prev_page` â€“ Referring article
- `curr_page` â€“ Target article
- `type` â€“ Link category: internal (`link`), external (`external`), or missing (`other`)
- `n` â€“ Number of clicks

---

## ğŸ” Objectives

- Load and clean the raw clickstream data using PySpark
- Register the data as a Spark SQL temporary view
- Perform SQL-style queries to:
  - Identify popular landing pages by traffic source
  - Trace the most common paths between articles
  - Analyze referral patterns for specific pages (e.g., "London")
- Visualize and interpret key findings

---

## ğŸ“Š Example Analysis

**Query**: Calculate the sum of `click_count` grouped by `link_category`

```sql
SELECT link_category, SUM(click_count)
FROM data
GROUP BY link_category
ORDER BY SUM(click_count) DESC
```

**Sample Output**:
| Link Category | Total Clicks    |
|---------------|-----------------|
| external      | 3,248,677,856   |
| link          | 97,805,811      |
| other         | 9,338,172       |

---

## ğŸ§¼ Data Cleaning Steps

- Filter out rows with missing or irrelevant values (e.g., `target_page = "Hanging_Gardens_of_Babylon"`).
- Focus on internal navigation patterns (`type = "link"`).
- Normalize column names where necessary.

---

## ğŸ§  Lessons Learned

- PySpark SQL is a powerful tool for querying large datasets efficiently.
- Wikipedia clickstream data provides insights into how users navigate knowledge.
- Cleaning and transforming semi-structured data is a critical step before analysis.

---

## ğŸ“¦ How to Run

1. Clone the repository:
   ```bash
   git clone https://github.com/kchabau/Analyzing_Wikipedia_Clickstreams_with_PySpark.git
   cd wiki-clickstream-pyspark
   ```

2. Launch PySpark shell or Jupyter environment.

3. Load the dataset and follow the notebook or script provided.